{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90082bf4-760e-4086-ad69-fa34ab133614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "torch.set_default_device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e92d390-e96f-40e2-aa7e-86cbc90d4753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.30\n",
      "  Using cached transformers-4.30.0-py3-none-any.whl (7.2 MB)\n",
      "Requirement already satisfied: filelock in c:\\users\\logis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.30) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\logis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.30) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\logis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.30) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\logis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.30) (22.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\logis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.30) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\logis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.30) (2023.8.8)\n",
      "Requirement already satisfied: requests in c:\\users\\logis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.30) (2.30.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\logis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.30) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\logis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.30) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\logis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.30) (4.66.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\logis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\logis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30) (4.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\logis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers==4.30) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\logis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers==4.30) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\logis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers==4.30) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\logis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers==4.30) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\logis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers==4.30) (2022.12.7)\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.35.2\n",
      "    Uninstalling transformers-4.35.2:\n",
      "      Successfully uninstalled transformers-4.35.2\n",
      "Successfully installed transformers-4.30.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4acfa4b1-fb5b-49b3-97a4-6557576d0ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "#Load the dataset from the HuggingFace Hub\n",
    "rd_ds = load_dataset(\"xiyuez/red-dot-design-award-product-description\")\n",
    "\n",
    "#Convert to pandas dataframe for convenient processing\n",
    "rd_df = pd.DataFrame(rd_ds['train'])\n",
    "\n",
    "#Combine the two attributes into an instruction string\n",
    "rd_df['instruction'] = 'Create a detailed description for the following product: '+ rd_df['product']+', belonging to category: '+ rd_df['category']\n",
    "\n",
    "rd_df = rd_df[['instruction', 'description']]\n",
    "\n",
    "#Get a 5000 sample subset for fine-tuning purposes\n",
    "rd_df_sample = rd_df.sample(n=5000, random_state=42)\n",
    "\n",
    "#Define template and format data into the template for supervised fine-tuning\n",
    "template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "\n",
    "{}\n",
    "\n",
    "### Response:\\n\"\"\"\n",
    "\n",
    "rd_df_sample['prompt'] = rd_df_sample[\"instruction\"].apply(lambda x: template.format(x))\n",
    "rd_df_sample.rename(columns={'description': 'response'}, inplace=True)\n",
    "rd_df_sample['response'] = rd_df_sample['response'] + \"\\n### End\"\n",
    "rd_df_sample = rd_df_sample[['prompt', 'response']]\n",
    "\n",
    "rd_df_sample['text'] = rd_df_sample[\"prompt\"] + rd_df_sample[\"response\"]\n",
    "rd_df_sample.drop(columns=['prompt', 'response'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ac9cbbb-7098-4d2b-a987-3e96143a56c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\logis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading pytorch_model.bin: 100%|██████████| 6.85G/6.85G [17:40<00:00, 6.46MB/s]\n",
      "C:\\Users\\logis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\logis\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\logis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenlm-research/open_llama_3b_v2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m LlamaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path)\n\u001b[1;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#Pass in a prompt and infer with the model\u001b[39;00m\n\u001b[0;32m     11\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ: Create a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: Optical Mouse\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mA:\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:2819\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2815\u001b[0m         device_map_without_lm_head \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   2816\u001b[0m             key: device_map[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m modules_to_not_convert\n\u001b[0;32m   2817\u001b[0m         }\n\u001b[0;32m   2818\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m-> 2819\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2820\u001b[0m \u001b[38;5;250m                \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2821\u001b[0m \u001b[38;5;124;03m                Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\u001b[39;00m\n\u001b[0;32m   2822\u001b[0m \u001b[38;5;124;03m                the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\u001b[39;00m\n\u001b[0;32m   2823\u001b[0m \u001b[38;5;124;03m                these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\u001b[39;00m\n\u001b[0;32m   2824\u001b[0m \u001b[38;5;124;03m                `device_map` to `from_pretrained`. Check\u001b[39;00m\n\u001b[0;32m   2825\u001b[0m \u001b[38;5;124;03m                https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\u001b[39;00m\n\u001b[0;32m   2826\u001b[0m \u001b[38;5;124;03m                for more details.\u001b[39;00m\n\u001b[0;32m   2827\u001b[0m \u001b[38;5;124;03m                \"\"\"\u001b[39;00m\n\u001b[0;32m   2828\u001b[0m             )\n\u001b[0;32m   2829\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m device_map_without_lm_head\n\u001b[0;32m   2831\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: \n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "model_path = 'openlm-research/open_llama_3b_v2'\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "model_path, load_in_8bit=True, device_map='auto',\n",
    ")\n",
    "\n",
    "#Pass in a prompt and infer with the model\n",
    "prompt = 'Q: Create a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: Optical Mouse\\nA:'\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "generation_output = model.generate(\n",
    "input_ids=input_ids, max_new_tokens=128\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb8348c-ea44-4d23-ad1b-52310e1aa1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt= \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Create a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: Optical Mouse\n",
    "\n",
    "### Response:\"\"\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "generation_output = model.generate(\n",
    "input_ids=input_ids, max_new_tokens=128\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9f12fd-cb94-4bae-96a6-e34a38b6c53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "...\n",
    "...\n",
    "\n",
    "#If only targeting attention blocks of the model\n",
    "target_modules = [\"q_proj\", \"v_proj\"]\n",
    "\n",
    "#If targeting all linear layers\n",
    "target_modules = ['q_proj','k_proj','v_proj','o_proj','gate_proj','down_proj','up_proj','lm_head']\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "r=16,\n",
    "target_modules = target_modules,\n",
    "lora_alpha=8,\n",
    "lora_dropout=0.05,\n",
    "bias=\"none\",\n",
    "task_type=\"CAUSAL_LM\",}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190a0cc1-b5b6-4b7f-985c-6287cf80a629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "model_modules = str(model.modules)\n",
    "pattern = r'\\((\\w+)\\): Linear'\n",
    "linear_layer_names = re.findall(pattern, model_modules)\n",
    "\n",
    "names = []\n",
    "# Print the names of the Linear layers\n",
    "for name in linear_layer_names:\n",
    "    names.append(name)\n",
    "target_modules = list(set(names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
