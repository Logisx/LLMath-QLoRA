{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "MODEL_CONFIG_FILE_PATH = os.environ['MODEL_CONFIG_FILE_PATH']\n",
    "MODEL_PARAMS_FILE_PATH = os.environ['MODEL_PARAMS_FILE_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelPredictionConfig:\n",
    "    data_path: Path\n",
    "    base_model: str\n",
    "    adapters_path: Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelPredictionParameters:\n",
    "    length_penalty: float\n",
    "    num_beams: int\n",
    "    max_length: int\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class BitsAndBytesParameters:\n",
    "    load_in_4bit: bool\n",
    "    bnb_4bit_quant_type: str\n",
    "    bnb_4bit_use_double_quant: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.common import read_yaml\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                model_config_filepath = MODEL_CONFIG_FILE_PATH,\n",
    "                model_params_filepath = MODEL_PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(Path(model_config_filepath))\n",
    "        self.params = read_yaml(Path(model_params_filepath))\n",
    "\n",
    "\n",
    "    def get_model_prediction_config(self) -> ModelPredictionConfig:\n",
    "        config = self.config.model_prediction\n",
    "\n",
    "        model_prediction_config = ModelPredictionConfig(\n",
    "            data_path=config.data_path,\n",
    "            base_model = config.base_model,\n",
    "            adapters_path = config.adapters_path\n",
    "        )\n",
    "\n",
    "        return model_prediction_config\n",
    "    \n",
    "    def get_bits_and_bytes_params(self) -> BitsAndBytesParameters:\n",
    "        params = self.params.bits_and_bytes_parameters\n",
    "\n",
    "        bits_and_bytes_parameters = BitsAndBytesParameters(\n",
    "            load_in_4bit = params.load_in_4bit,\n",
    "            bnb_4bit_quant_type = params.bnb_4bit_quant_type,\n",
    "            bnb_4bit_use_double_quant = params.bnb_4bit_use_double_quant\n",
    "        )\n",
    "\n",
    "        return bits_and_bytes_parameters\n",
    "        \n",
    "    def get_model_prediction_parameters(self) -> ModelPredictionParameters:\n",
    "        config = self.params.prediction_parameters\n",
    "\n",
    "        model_prediction_parameters = ModelPredictionParameters(\n",
    "            length_penalty=config.length_penalty,\n",
    "            num_beams = config.num_beams,\n",
    "            max_length = config.max_length\n",
    "        )\n",
    "\n",
    "        return model_prediction_parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\logis\\Logis\\Study\\Self Edu\\Projects\\LLM-Instruction-tuning-School-math-questions\\.venv\\Lib\\site-packages\\bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, LlamaForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "from src.logging import logger\n",
    "\n",
    "class ModelPrediction:\n",
    "    def __init__(self, config: ModelPredictionConfig, bits_and_bytes_parameters: BitsAndBytesParameters, params: ModelPredictionParameters):\n",
    "        self.config = config\n",
    "        self.bits_and_bytes_parameters = bits_and_bytes_parameters\n",
    "        self.params = params \n",
    "\n",
    "    def __initialize_tokenizer(self, model_name: str):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.base_model)\n",
    "        self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        logger.info(\"Tokenizer initialized\")\n",
    "\n",
    "    def __initialize_bits_and_bytes(self):\n",
    "        self.nf4_config = BitsAndBytesConfig(\n",
    "            load_in_4bit = self.bits_and_bytes_parameters.load_in_4bit,\n",
    "            bnb_4bit_quant_type = self.bits_and_bytes_parameters.bnb_4bit_quant_type,\n",
    "            bnb_4bit_use_double_quant = self.bits_and_bytes_parameters.bnb_4bit_use_double_quant,\n",
    "            bnb_4bit_compute_dtype = torch.bfloat16\n",
    "        )\n",
    "        logger.info(\"Bits and bytes initialized\")\n",
    "\n",
    "    def __initialize_model(self):\n",
    "        self.model = LlamaForCausalLM.from_pretrained(self.config.base_model, device_map='auto', quantization_config=self.nf4_config)\n",
    "        self.peft_model = PeftModel.from_pretrained(self.model, self.config.adapters_path)\n",
    "        logger.info(\"Model initialized\")\n",
    "\n",
    "\n",
    "    def predict(self, question):\n",
    "        self.__initialize_tokenizer(self.config.base_model)\n",
    "        self.__initialize_bits_and_bytes()\n",
    "        self.__initialize_model()\n",
    "\n",
    "        gen_kwargs = {\"length_penalty\": self.params.length_penalty,\n",
    "                      \"num_beams\": self.params.max_length,\n",
    "                      \"max_length\": self.params.max_length}\n",
    "\n",
    "        pipe = pipeline(\"generation\", model=self.peft_model, tokenizer=self.tokenizer)\n",
    "        logger.info(\"Pipeline initialized\")\n",
    "\n",
    "        logger.info(\"Generating output...\")\n",
    "        output = pipe(question, **gen_kwargs)[0][\"response\"]\n",
    "        logger.info(\"Output generated: \", output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-01 17:43:55,182: INFO: common: yaml file: config\\model-config.yaml loaded successfully]\n",
      "[2023-12-01 17:43:55,187: INFO: common: yaml file: config\\model-parameters.yaml loaded successfully]\n",
      "[2023-12-01 17:49:36,752: INFO: 1428695309: Tokenizer initialized]\n",
      "[2023-12-01 17:49:36,756: INFO: 1428695309: Bits and bytes initialized]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\logis\\Logis\\Study\\Self Edu\\Projects\\LLM-Instruction-tuning-School-math-questions\\notebooks\\05_model_prediction.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/05_model_prediction.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     model_prediction\u001b[39m.\u001b[39mpredict(question)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/05_model_prediction.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/05_model_prediction.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "\u001b[1;32mc:\\Users\\logis\\Logis\\Study\\Self Edu\\Projects\\LLM-Instruction-tuning-School-math-questions\\notebooks\\05_model_prediction.ipynb Cell 6\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/05_model_prediction.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     bits_and_bytes_parameters \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mget_bits_and_bytes_params()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/05_model_prediction.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     model_prediction \u001b[39m=\u001b[39m ModelTrainer(config\u001b[39m=\u001b[39mmodel_prediction_config, bits_and_bytes_parameters\u001b[39m=\u001b[39mbits_and_bytes_parameters, params\u001b[39m=\u001b[39mmodel_prediction_parameters)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/05_model_prediction.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     model_prediction\u001b[39m.\u001b[39;49mpredict(question)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/05_model_prediction.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/05_model_prediction.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "\u001b[1;32mc:\\Users\\logis\\Logis\\Study\\Self Edu\\Projects\\LLM-Instruction-tuning-School-math-questions\\notebooks\\05_model_prediction.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/05_model_prediction.ipynb#W5sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__initialize_tokenizer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mbase_model)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/05_model_prediction.ipynb#W5sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__initialize_bits_and_bytes()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/05_model_prediction.ipynb#W5sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__initialize_model()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/05_model_prediction.ipynb#W5sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m gen_kwargs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mlength_penalty\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams\u001b[39m.\u001b[39mlength_penalty,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/05_model_prediction.ipynb#W5sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m               \u001b[39m\"\u001b[39m\u001b[39mnum_beams\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams\u001b[39m.\u001b[39mmax_length,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/05_model_prediction.ipynb#W5sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m               \u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams\u001b[39m.\u001b[39mmax_length}\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/05_model_prediction.ipynb#W5sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m pipe \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39m\u001b[39mgeneration\u001b[39m\u001b[39m\"\u001b[39m, model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpeft_model, tokenizer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer)\n",
      "\u001b[1;32mc:\\Users\\logis\\Logis\\Study\\Self Edu\\Projects\\LLM-Instruction-tuning-School-math-questions\\notebooks\\05_model_prediction.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/05_model_prediction.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__initialize_model\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/05_model_prediction.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m LlamaForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mbase_model, device_map\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m'\u001b[39;49m, quantization_config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnf4_config)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/05_model_prediction.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpeft_model \u001b[39m=\u001b[39m PeftModel\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39madapters_path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/05_model_prediction.ipynb#W5sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mModel initialized\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\logis\\Logis\\Study\\Self Edu\\Projects\\LLM-Instruction-tuning-School-math-questions\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:2819\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2815\u001b[0m         device_map_without_lm_head \u001b[39m=\u001b[39m {\n\u001b[0;32m   2816\u001b[0m             key: device_map[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m device_map\u001b[39m.\u001b[39mkeys() \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m modules_to_not_convert\n\u001b[0;32m   2817\u001b[0m         }\n\u001b[0;32m   2818\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m device_map_without_lm_head\u001b[39m.\u001b[39mvalues() \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdisk\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m device_map_without_lm_head\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m-> 2819\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2820\u001b[0m \u001b[39m                \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2821\u001b[0m \u001b[39m                Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\u001b[39;00m\n\u001b[0;32m   2822\u001b[0m \u001b[39m                the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\u001b[39;00m\n\u001b[0;32m   2823\u001b[0m \u001b[39m                these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\u001b[39;00m\n\u001b[0;32m   2824\u001b[0m \u001b[39m                `device_map` to `from_pretrained`. Check\u001b[39;00m\n\u001b[0;32m   2825\u001b[0m \u001b[39m                https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\u001b[39;00m\n\u001b[0;32m   2826\u001b[0m \u001b[39m                for more details.\u001b[39;00m\n\u001b[0;32m   2827\u001b[0m \u001b[39m                \"\"\"\u001b[39;00m\n\u001b[0;32m   2828\u001b[0m             )\n\u001b[0;32m   2829\u001b[0m         \u001b[39mdel\u001b[39;00m device_map_without_lm_head\n\u001b[0;32m   2831\u001b[0m \u001b[39melif\u001b[39;00m device_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: \n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    question = \"What is 2+2?\"\n",
    "    config = ConfigurationManager()\n",
    "    model_prediction_config = config.get_model_prediction_config()\n",
    "    model_prediction_parameters = config.get_model_prediction_parameters()\n",
    "    bits_and_bytes_parameters = config.get_bits_and_bytes_params()\n",
    "    model_prediction = ModelPrediction(config=model_prediction_config, bits_and_bytes_parameters=bits_and_bytes_parameters, params=model_prediction_parameters)\n",
    "    model_prediction.predict(question)\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
