{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "MODEL_CONFIG_FILE_PATH = os.environ['MODEL_CONFIG_FILE_PATH']\n",
    "MODEL_PARAMS_FILE_PATH = os.environ['MODEL_PARAMS_FILE_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelPredictionConfig:\n",
    "    data_path: Path\n",
    "    base_model: str\n",
    "    adapters_path: Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelPredictionParameters:\n",
    "    length_penalty: float\n",
    "    num_beams: int\n",
    "    max_length: int\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class BitsAndBytesParameters:\n",
    "    load_in_4bit: bool\n",
    "    bnb_4bit_quant_type: str\n",
    "    bnb_4bit_use_double_quant: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.common import read_yaml\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                model_config_filepath = MODEL_CONFIG_FILE_PATH,\n",
    "                model_params_filepath = MODEL_PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(Path(model_config_filepath))\n",
    "        self.params = read_yaml(Path(model_params_filepath))\n",
    "\n",
    "\n",
    "    def get_model_prediction_config(self) -> ModelPredictionConfig:\n",
    "        config = self.config.model_prediction\n",
    "\n",
    "        model_prediction_config = ModelPredictionConfig(\n",
    "            data_path=config.data_path,\n",
    "            base_model = config.base_model,\n",
    "            adapters_path = config.adapters_path\n",
    "        )\n",
    "\n",
    "        return model_prediction_config\n",
    "    \n",
    "    def get_bits_and_bytes_params(self) -> BitsAndBytesParameters:\n",
    "        params = self.params.bits_and_bytes_parameters\n",
    "\n",
    "        bits_and_bytes_parameters = BitsAndBytesParameters(\n",
    "            load_in_4bit = params.load_in_4bit,\n",
    "            bnb_4bit_quant_type = params.bnb_4bit_quant_type,\n",
    "            bnb_4bit_use_double_quant = params.bnb_4bit_use_double_quant\n",
    "        )\n",
    "\n",
    "        return bits_and_bytes_parameters\n",
    "        \n",
    "    def get_model_prediction_parameters(self) -> ModelPredictionParameters:\n",
    "        config = self.params.prediction_parameters\n",
    "\n",
    "        model_prediction_parameters = ModelPredictionParameters(\n",
    "            length_penalty=config.length_penalty,\n",
    "            num_beams = config.num_beams,\n",
    "            max_length = config.max_length\n",
    "        )\n",
    "\n",
    "        return model_prediction_parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, LlamaForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "from src.logging import logger\n",
    "\n",
    "class ModelPrediction:\n",
    "    def __init__(self, config: ModelPredictionConfig, bits_and_bytes_parameters: BitsAndBytesParameters, params: ModelPredictionParameters):\n",
    "        self.config = config\n",
    "        self.bits_and_bytes_parameters = bits_and_bytes_parameters\n",
    "        self.params = params \n",
    "\n",
    "    def __initialize_tokenizer(self, model_name: str):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.base_model)\n",
    "        self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        logger.info(\"Tokenizer initialized\")\n",
    "\n",
    "    def __initialize_bits_and_bytes(self):\n",
    "        self.nf4_config = BitsAndBytesConfig(\n",
    "            load_in_4bit = self.bits_and_bytes_parameters.load_in_4bit,\n",
    "            bnb_4bit_quant_type = self.bits_and_bytes_parameters.bnb_4bit_quant_type,\n",
    "            bnb_4bit_use_double_quant = self.bits_and_bytes_parameters.bnb_4bit_use_double_quant,\n",
    "            bnb_4bit_compute_dtype = torch.bfloat16\n",
    "        )\n",
    "        logger.info(\"Bits and bytes initialized\")\n",
    "\n",
    "    def __initialize_model(self):\n",
    "        self.model = LlamaForCausalLM.from_pretrained(self.config.base_model, device_map='auto', quantization_config=self.nf4_config)\n",
    "        self.peft_model = PeftModel.from_pretrained(self.model, self.config.adapters_path)\n",
    "        logger.info(\"Model initialized\")\n",
    "\n",
    "\n",
    "    def predict(self, question):\n",
    "        self.__initialize_tokenizer(self.config.base_model)\n",
    "        self.__initialize_bits_and_bytes()\n",
    "        self.__initialize_model()\n",
    "\n",
    "        gen_kwargs = {\"length_penalty\": self.params.length_penalty,\n",
    "                      \"num_beams\": self.params.max_length,\n",
    "                      \"max_length\": self.params.max_length}\n",
    "\n",
    "        pipe = pipeline(\"generation\", model=self.peft_model, tokenizer=self.tokenizer)\n",
    "        logger.info(\"Pipeline initialized\")\n",
    "\n",
    "        logger.info(\"Generating output...\")\n",
    "        output = pipe(question, **gen_kwargs)[0][\"response\"]\n",
    "        logger.info(\"Output generated: \", output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    question = \"What is 2+2?\"\n",
    "    config = ConfigurationManager()\n",
    "    model_prediction_config = config.get_model_prediction_config()\n",
    "    model_prediction_parameters = config.get_model_prediction_parameters()\n",
    "    bits_and_bytes_parameters = config.get_bits_and_bytes_params()\n",
    "    model_prediction = ModelPrediction(config=model_prediction_config, bits_and_bytes_parameters=bits_and_bytes_parameters, params=model_prediction_parameters)\n",
    "    model_prediction.predict(question)\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
