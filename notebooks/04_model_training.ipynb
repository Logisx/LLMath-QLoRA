{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "MODEL_CONFIG_FILE_PATH = os.environ['MODEL_CONFIG_FILE_PATH']\n",
    "MODEL_PARAMS_FILE_PATH = os.environ['MODEL_PARAMS_FILE_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    base_model: str\n",
    "    training_name: str\n",
    "    upload_from_hf: bool\n",
    "    hf_model_name: str\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class LoraParameters:\n",
    "    r: int\n",
    "    target_modules: list\n",
    "    lora_alpha: float\n",
    "    lora_dropout: float\n",
    "    bias: str\n",
    "    task_type: str\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class BitsAndBytesParameters:\n",
    "    load_in_4bit: bool\n",
    "    bnb_4bit_quant_type: str\n",
    "    bnb_4bit_use_double_quant: bool\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingArgumentsParameters:\n",
    "    output_dir: str\n",
    "    evaluation_strategy: str\n",
    "    save_strategy: str\n",
    "    num_train_epochs: float\n",
    "    per_device_train_batch_size: int\n",
    "    gradient_accumulation_steps: int\n",
    "    optim: str\n",
    "    learning_rate: float\n",
    "    fp16: bool\n",
    "    max_grad_norm: float\n",
    "    warmup_ratio: float\n",
    "    group_by_length: bool\n",
    "    lr_scheduler_type: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.common import read_yaml\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                model_config_filepath = MODEL_CONFIG_FILE_PATH,\n",
    "                model_params_filepath = MODEL_PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(Path(model_config_filepath))\n",
    "        self.params = read_yaml(Path(model_params_filepath))\n",
    "\n",
    "\n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir = Path(config.root_dir),\n",
    "            data_path = Path(config.data_path),\n",
    "            base_model = config.base_model,\n",
    "            training_name = config.training_name,\n",
    "            upload_from_hf = config.upload_from_hf,\n",
    "            hf_model_name = config.hf_model_name\n",
    "        )\n",
    "\n",
    "        return model_trainer_config\n",
    "    \n",
    "\n",
    "    def get_lora_params(self) -> LoraParameters:\n",
    "        params = self.params.lora_parameters\n",
    "\n",
    "        lora_parameters = LoraParameters(\n",
    "            r = params.r,\n",
    "            target_modules = params.target_modules,\n",
    "            lora_alpha = params.lora_alpha,\n",
    "            lora_dropout = params.lora_dropout,\n",
    "            bias = params.bias,\n",
    "            task_type = params.task_type\n",
    "        )\n",
    "\n",
    "        return lora_parameters\n",
    "    \n",
    "\n",
    "    def get_bits_and_bytes_params(self) -> BitsAndBytesParameters:\n",
    "        params = self.params.bits_and_bytes_parameters\n",
    "\n",
    "        bits_and_bytes_parameters = BitsAndBytesParameters(\n",
    "            load_in_4bit = params.load_in_4bit,\n",
    "            bnb_4bit_quant_type = params.bnb_4bit_quant_type,\n",
    "            bnb_4bit_use_double_quant = params.bnb_4bit_use_double_quant\n",
    "        )\n",
    "\n",
    "        return bits_and_bytes_parameters\n",
    "    \n",
    "\n",
    "    def get_training_args(self) -> TrainingArgumentsParameters:\n",
    "        params = self.params.training_arguments\n",
    "\n",
    "        training_args = TrainingArgumentsParameters(\n",
    "            output_dir = params.output_dir,\n",
    "            evaluation_strategy = params.evaluation_strategy,\n",
    "            save_strategy = params.save_strategy,\n",
    "            num_train_epochs = params.num_train_epochs,\n",
    "            per_device_train_batch_size = params.per_device_train_batch_size,\n",
    "            gradient_accumulation_steps = params.gradient_accumulation_steps,\n",
    "            optim = params.optim,\n",
    "            learning_rate = params.learning_rate,\n",
    "            fp16 = params.fp16,\n",
    "            max_grad_norm = params.max_grad_norm,\n",
    "            warmup_ratio = params.warmup_ratio,\n",
    "            group_by_length = params.group_by_length,\n",
    "            lr_scheduler_type = params.lr_scheduler_type\n",
    "        )\n",
    "\n",
    "        return training_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\logis\\Logis\\Study\\Self Edu\\Projects\\LLM-Instruction-tuning-School-math-questions\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:149: UserWarning: Field \"model_server_url\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\logis\\Logis\\Study\\Self Edu\\Projects\\LLM-Instruction-tuning-School-math-questions\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_config.py:321: UserWarning: Valid config keys have changed in V2:\n",
      "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "  warnings.warn(message, UserWarning)\n",
      "c:\\Users\\logis\\Logis\\Study\\Self Edu\\Projects\\LLM-Instruction-tuning-School-math-questions\\.venv\\Lib\\site-packages\\bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\logis\\Logis\\Study\\Self Edu\\Projects\\LLM-Instruction-tuning-School-math-questions\\.venv\\Lib\\site-packages\\trl\\trainer\\ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import locale\n",
    "import math\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM, TrainingArguments, BitsAndBytesConfig\n",
    "from peft import get_peft_model, LoraConfig, PeftModel\n",
    "\n",
    "from src.logging import logger\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, model_trainer_config: ModelTrainerConfig, lora_parameters: LoraParameters, bits_and_bytes_parameters: BitsAndBytesParameters, training_arguments: TrainingArgumentsParameters):\n",
    "        self.model_trainer_config = model_trainer_config\n",
    "        self.lora_parameters = lora_parameters\n",
    "        self.bits_and_bytes_parameters = bits_and_bytes_parameters\n",
    "        self.training_arguments = training_arguments\n",
    "\n",
    "\n",
    "    def __load_data(self):\n",
    "        train_dataset = pd.read_csv(os.path.join(self.model_trainer_config.data_path, \"train_dataset.csv\"))\n",
    "        eval_dataset = pd.read_csv(os.path.join(self.model_trainer_config.data_path, \"eval_dataset.csv\"))\n",
    "\n",
    "        train_dataset = Dataset.from_pandas(train_dataset)\n",
    "        eval_dataset = Dataset.from_pandas(eval_dataset)\n",
    "\n",
    "        self.train_dataset = train_dataset\n",
    "        self.eval_dataset = eval_dataset\n",
    "        logger.info(\"Data loaded\")\n",
    "\n",
    "\n",
    "    def __initialize_tokenizer(self, model_name: str):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_trainer_config.base_model)\n",
    "        self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        logger.info(\"Tokenizer initialized\")\n",
    "\n",
    "\n",
    "    def __initialize_lora(self):\n",
    "        self.lora_cofig = LoraConfig(\n",
    "            r = self.lora_parameters.r,\n",
    "            target_modules = self.lora_parameters.target_modules,\n",
    "            lora_alpha = self.lora_parameters.lora_alpha,\n",
    "            lora_dropout = self.lora_parameters.lora_dropout,\n",
    "            bias = self.lora_parameters.bias,\n",
    "            task_type = self.lora_parameters.task_type\n",
    "        )\n",
    "        logger.info(\"Lora initialized\")\n",
    "\n",
    "\n",
    "    def __initialize_bits_and_bytes(self):\n",
    "        self.nf4_config = BitsAndBytesConfig(\n",
    "            load_in_4bit = self.bits_and_bytes_parameters.load_in_4bit,\n",
    "            bnb_4bit_quant_type = self.bits_and_bytes_parameters.bnb_4bit_quant_type,\n",
    "            bnb_4bit_use_double_quant = self.bits_and_bytes_parameters.bnb_4bit_use_double_quant,\n",
    "            bnb_4bit_compute_dtype = torch.bfloat16\n",
    "        )\n",
    "        logger.info(\"Bits and bytes initialized\")\n",
    "    \n",
    "\n",
    "    def __initialize_training_arguments(self):\n",
    "        self.training_args = TrainingArguments(\n",
    "            output_dir = self.training_arguments.output_dir,\n",
    "            evaluation_strategy = self.training_arguments.evaluation_strategy,\n",
    "            save_strategy = self.training_arguments.save_strategy,\n",
    "            num_train_epochs = self.training_arguments.num_train_epochs,\n",
    "            per_device_train_batch_size = self.training_arguments.per_device_train_batch_size,\n",
    "            gradient_accumulation_steps = self.training_arguments.gradient_accumulation_steps,\n",
    "            optim = self.training_arguments.optim,\n",
    "            learning_rate = self.training_arguments.learning_rate,\n",
    "            fp16 = self.training_arguments.fp16,\n",
    "            max_grad_norm = self.training_arguments.max_grad_norm,\n",
    "            warmup_ratio = self.training_arguments.warmup_ratio,\n",
    "            group_by_length = self.training_arguments.group_by_length,\n",
    "            lr_scheduler_type = self.training_arguments.lr_scheduler_type\n",
    "        )\n",
    "        logger.info(\"Training arguments initialized\")\n",
    "\n",
    "\n",
    "    def __create_model(self):\n",
    "        self.model = LlamaForCausalLM.from_pretrained(\n",
    "            self.model_trainer_config.base_model, device_map='auto', quantization_config=self.nf4_config,\n",
    "        )\n",
    "        self.model = get_peft_model(self.model, self.lora_config)\n",
    "        #self.model.print_trainable_parameters()\n",
    "        logger.info(\"Model created\")\n",
    "\n",
    "    def __evaluate(self, trainer):\n",
    "        evaluation_results = trainer.evaluate()\n",
    "        logger.info(f\"Perplexity: {math.exp(evaluation_results['eval_loss']):.2f}\")\n",
    "\n",
    "    def __save_model(self, model):\n",
    "        model.save_pretrained(os.path.join(self.config.root_dir, f\"{self.model_trainer_config.training_name}-math-adapters\"))\n",
    "        logger.info(\"Model saved\")\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        if self.model_trainer_config.upload_from_hf:\n",
    "            logger.info(\"Uploading model from HuggingFace\")\n",
    "            self.__initialize_tokenizer(self.model_trainer_config.base_model)\n",
    "            self.__initialize_bits_and_bytes()\n",
    "            \n",
    "            model = AutoModelForCausalLM.from_pretrained(self.model_trainer_config.hf_model_name,\n",
    "                                                              device_map='auto',\n",
    "                                                              quantization_config=self.nf4_config)\n",
    "            \n",
    "            peft_model = PeftModel.from_pretrained(model,\n",
    "                                                    self.model_trainer_config.hf_model_name)\n",
    "            logger.info(\"Model uploaded\")\n",
    "            \n",
    "            self.__save_model(peft_model)\n",
    "\n",
    "            return None\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            try:\n",
    "                locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "                \n",
    "                self.__load_data()\n",
    "                self.__initialize_tokenizer(self.model_trainer_config.base_model)\n",
    "                self.__initialize_lora()\n",
    "                self.__initialize_bits_and_bytes()\n",
    "                self.__initialize_training_arguments()\n",
    "                self.__create_model()\n",
    "\n",
    "                trainer = SFTTrainer(self.model,\n",
    "                                    train_dataset=self.train_dataset,\n",
    "                                    eval_dataset=self.eval_dataset,\n",
    "                                    dataset_text_field=\"text\",\n",
    "                                    max_seq_length=256,\n",
    "                                    args=self.training_args,\n",
    "                                    )\n",
    "                logger.info(\"Trainer created\")\n",
    "                \n",
    "                #Upcast layer norms to float 32 for stability\n",
    "                for name, module in trainer.model.named_modules():\n",
    "                    if \"norm\" in name:\n",
    "                        module = module.to(torch.float32)\n",
    "                logger.info(\"Layer norms upcasted to float32\")\n",
    "                \n",
    "                logger.info(\">>>>>>> Training started <<<<<<<<\")\n",
    "                with mlflow.start_run(run_name=self.model_trainer_config.training_name):\n",
    "                    trainer.train()\n",
    "                logger.info(\">>>>>>> Training completed <<<<<<<<\")\n",
    "\n",
    "                self.__evaluate(trainer)\n",
    "                \n",
    "                self.__save_model(self.model)\n",
    "\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "        else:\n",
    "            raise Exception(\"No GPU found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-01 16:51:01,064: INFO: common: yaml file: config\\model-config.yaml loaded successfully]\n",
      "[2023-12-01 16:51:01,076: INFO: common: yaml file: config\\model-parameters.yaml loaded successfully]\n",
      "[2023-12-01 16:51:01,077: INFO: 2242578423: Uploading model from HuggingFace]\n",
      "[2023-12-01 16:55:41,060: INFO: 2242578423: Tokenizer initialized]\n",
      "[2023-12-01 16:55:41,063: INFO: 2242578423: Bits and bytes initialized]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Logisx/open_llama_3b_v2-Fine-Tuned-Grade_School_Math_Instructions does not appear to have a file named config.json. Checkout 'https://huggingface.co/Logisx/open_llama_3b_v2-Fine-Tuned-Grade_School_Math_Instructions/main' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\logis\\Logis\\Study\\Self Edu\\Projects\\LLM-Instruction-tuning-School-math-questions\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py:270\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 270\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[0;32m    271\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\logis\\Logis\\Study\\Self Edu\\Projects\\LLM-Instruction-tuning-School-math-questions\\.venv\\Lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/Logisx/open_llama_3b_v2-Fine-Tuned-Grade_School_Math_Instructions/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mEntryNotFoundError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\logis\\Logis\\Study\\Self Edu\\Projects\\LLM-Instruction-tuning-School-math-questions\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:417\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    416\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 417\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    418\u001b[0m         path_or_repo_id,\n\u001b[0;32m    419\u001b[0m         filename,\n\u001b[0;32m    420\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    421\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[0;32m    422\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    423\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    424\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    425\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    426\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    427\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    428\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    429\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    430\u001b[0m     )\n\u001b[0;32m    432\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[1;32mc:\\Users\\logis\\Logis\\Study\\Self Edu\\Projects\\LLM-Instruction-tuning-School-math-questions\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\logis\\Logis\\Study\\Self Edu\\Projects\\LLM-Instruction-tuning-School-math-questions\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1247\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[0;32m   1246\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1247\u001b[0m     metadata \u001b[39m=\u001b[39m get_hf_file_metadata(\n\u001b[0;32m   1248\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m   1249\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[0;32m   1250\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   1251\u001b[0m         timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[0;32m   1252\u001b[0m     )\n\u001b[0;32m   1253\u001b[0m \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n\u001b[0;32m   1254\u001b[0m     \u001b[39m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\logis\\Logis\\Study\\Self Edu\\Projects\\LLM-Instruction-tuning-School-math-questions\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\logis\\Logis\\Study\\Self Edu\\Projects\\LLM-Instruction-tuning-School-math-questions\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1624\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[0;32m   1623\u001b[0m \u001b[39m# Retrieve metadata\u001b[39;00m\n\u001b[1;32m-> 1624\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[0;32m   1625\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1626\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m   1627\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m   1628\u001b[0m     allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1629\u001b[0m     follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1630\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   1631\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m   1632\u001b[0m )\n\u001b[0;32m   1633\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[1;32mc:\\Users\\logis\\Logis\\Study\\Self Edu\\Projects\\LLM-Instruction-tuning-School-math-questions\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:402\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[39mif\u001b[39;00m follow_relative_redirects:\n\u001b[1;32m--> 402\u001b[0m     response \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[0;32m    403\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[0;32m    404\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    405\u001b[0m         follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    406\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams,\n\u001b[0;32m    407\u001b[0m     )\n\u001b[0;32m    409\u001b[0m     \u001b[39m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[0;32m    410\u001b[0m     \u001b[39m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\logis\\Logis\\Study\\Self Edu\\Projects\\LLM-Instruction-tuning-School-math-questions\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:426\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    425\u001b[0m response \u001b[39m=\u001b[39m get_session()\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[1;32m--> 426\u001b[0m hf_raise_for_status(response)\n\u001b[0;32m    427\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\logis\\Logis\\Study\\Self Edu\\Projects\\LLM-Instruction-tuning-School-math-questions\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py:280\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    279\u001b[0m     message \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Client Error.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEntry Not Found for url: \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39murl\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 280\u001b[0m     \u001b[39mraise\u001b[39;00m EntryNotFoundError(message, response) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[39melif\u001b[39;00m error_code \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGatedRepo\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[1;31mEntryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-6569f3ed-5cdf8ff26c9de907124b02a5;39351df0-72df-44ab-a5de-424a26ca5102)\n\nEntry Not Found for url: https://huggingface.co/Logisx/open_llama_3b_v2-Fine-Tuned-Grade_School_Math_Instructions/resolve/main/config.json.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\logis\\Logis\\Study\\Self Edu\\Projects\\LLM-Instruction-tuning-School-math-questions\\notebooks\\04_model_training.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/04_model_training.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     model_trainer\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/04_model_training.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/04_model_training.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "\u001b[1;32mc:\\Users\\logis\\Logis\\Study\\Self Edu\\Projects\\LLM-Instruction-tuning-School-math-questions\\notebooks\\04_model_training.ipynb Cell 6\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/04_model_training.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     training_args \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mget_training_args()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/04_model_training.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     model_trainer \u001b[39m=\u001b[39m ModelTrainer(model_trainer_config\u001b[39m=\u001b[39mmodel_trainer_config, lora_parameters\u001b[39m=\u001b[39mlora_parameters, bits_and_bytes_parameters\u001b[39m=\u001b[39mbits_and_bytes_parameters, training_arguments\u001b[39m=\u001b[39mtraining_args)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/04_model_training.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     model_trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/04_model_training.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/04_model_training.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "\u001b[1;32mc:\\Users\\logis\\Logis\\Study\\Self Edu\\Projects\\LLM-Instruction-tuning-School-math-questions\\notebooks\\04_model_training.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/04_model_training.ipynb#W6sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__initialize_tokenizer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_trainer_config\u001b[39m.\u001b[39mbase_model)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/04_model_training.ipynb#W6sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__initialize_bits_and_bytes()\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/04_model_training.ipynb#W6sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_trainer_config\u001b[39m.\u001b[39;49mhf_model_name,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/04_model_training.ipynb#W6sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m                                                   device_map\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/04_model_training.ipynb#W6sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m                                                   quantization_config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnf4_config)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/04_model_training.ipynb#W6sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m peft_model \u001b[39m=\u001b[39m PeftModel\u001b[39m.\u001b[39mfrom_pretrained(model,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/04_model_training.ipynb#W6sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m                                         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_trainer_config\u001b[39m.\u001b[39mhf_model_name)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/logis/Logis/Study/Self%20Edu/Projects/LLM-Instruction-tuning-School-math-questions/notebooks/04_model_training.ipynb#W6sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mModel uploaded\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\logis\\Logis\\Study\\Self Edu\\Projects\\LLM-Instruction-tuning-School-math-questions\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:456\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[39mif\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtorch_dtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    454\u001b[0m     _ \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtorch_dtype\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 456\u001b[0m config, kwargs \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[0;32m    457\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m    458\u001b[0m     return_unused_kwargs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    459\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49mtrust_remote_code,\n\u001b[0;32m    460\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs,\n\u001b[0;32m    461\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    462\u001b[0m )\n\u001b[0;32m    464\u001b[0m \u001b[39m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[0;32m    465\u001b[0m \u001b[39mif\u001b[39;00m kwargs_orig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtorch_dtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\logis\\Logis\\Study\\Self Edu\\Projects\\LLM-Instruction-tuning-School-math-questions\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:944\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    942\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mname_or_path\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m pretrained_model_name_or_path\n\u001b[0;32m    943\u001b[0m trust_remote_code \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtrust_remote_code\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 944\u001b[0m config_dict, unused_kwargs \u001b[39m=\u001b[39m PretrainedConfig\u001b[39m.\u001b[39;49mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    945\u001b[0m has_remote_code \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mAutoConfig\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    946\u001b[0m has_local_code \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[1;32mc:\\Users\\logis\\Logis\\Study\\Self Edu\\Projects\\LLM-Instruction-tuning-School-math-questions\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:574\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    572\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[0;32m    573\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[1;32m--> 574\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    575\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[0;32m    576\u001b[0m     original_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\logis\\Logis\\Study\\Self Edu\\Projects\\LLM-Instruction-tuning-School-math-questions\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:629\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    625\u001b[0m configuration_file \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_configuration_file\u001b[39m\u001b[39m\"\u001b[39m, CONFIG_NAME)\n\u001b[0;32m    627\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[1;32m--> 629\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[0;32m    630\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m    631\u001b[0m         configuration_file,\n\u001b[0;32m    632\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    633\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    634\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    635\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    636\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    637\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    638\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    639\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    640\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[0;32m    641\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[0;32m    642\u001b[0m     )\n\u001b[0;32m    643\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m    644\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[0;32m    645\u001b[0m     \u001b[39m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[0;32m    646\u001b[0m     \u001b[39m# the original exception.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\logis\\Logis\\Study\\Self Edu\\Projects\\LLM-Instruction-tuning-School-math-questions\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:463\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[39mif\u001b[39;00m revision \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m         revision \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmain\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    464\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m does not appear to have a file named \u001b[39m\u001b[39m{\u001b[39;00mfull_filename\u001b[39m}\u001b[39;00m\u001b[39m. Checkout \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    465\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mrevision\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m for available files.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    466\u001b[0m     )\n\u001b[0;32m    467\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    468\u001b[0m     \u001b[39m# First we try to see if we have a cached version (not up to date):\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     resolved_file \u001b[39m=\u001b[39m try_to_load_from_cache(path_or_repo_id, full_filename, cache_dir\u001b[39m=\u001b[39mcache_dir, revision\u001b[39m=\u001b[39mrevision)\n",
      "\u001b[1;31mOSError\u001b[0m: Logisx/open_llama_3b_v2-Fine-Tuned-Grade_School_Math_Instructions does not appear to have a file named config.json. Checkout 'https://huggingface.co/Logisx/open_llama_3b_v2-Fine-Tuned-Grade_School_Math_Instructions/main' for available files."
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    lora_parameters = config.get_lora_params()\n",
    "    bits_and_bytes_parameters = config.get_bits_and_bytes_params()\n",
    "    training_args = config.get_training_args()\n",
    "    model_trainer = ModelTrainer(model_trainer_config=model_trainer_config, lora_parameters=lora_parameters, bits_and_bytes_parameters=bits_and_bytes_parameters, training_arguments=training_args)\n",
    "    model_trainer.train()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
